{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"mBGvTgoz9DTZ"},"outputs":[],"source":["# %pip install diffusers[\"torch\"] transformers\n","# %pip install torchmetrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1rWAUVtv-k64"},"outputs":[],"source":["# This mounts your Google Drive to the Colab VM.\n","# from google.colab import drive\n","# drive.mount('/content/drive')\n","# FOLDERNAME = 'stanford/cs231n/cs231n_project/'\n","# assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","# import sys\n","# sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10187,"status":"ok","timestamp":1717182082945,"user":{"displayName":"He Nan Li (Tony)","userId":"01291724548798117557"},"user_tz":420},"id":"hgKwk-JG9DTa","outputId":"cabca03d-ed0c-4dad-9ab1-6e1ed6d31bf7"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torch.utils.data import sampler\n","import matplotlib.pyplot as plt\n","\n","\n","import torchvision.datasets as dset\n","from torchvision.datasets import MNIST, CIFAR10\n","import torchvision.transforms as T\n","import torchvision.transforms as transforms\n","import torchvision.transforms.functional\n","from torchvision.transforms import v2\n","from torchvision.utils import save_image, make_grid\n","from torch.utils.data import DataLoader\n","from torch.optim import Adam\n","from torch.optim import AdamW\n","from tqdm import tqdm\n","import math\n","import torch.nn.functional as F\n","\n","import numpy as np\n","from torchmetrics.image.inception import InceptionScore\n","\n","import ddpm\n","import utils"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4906,"status":"ok","timestamp":1717182325820,"user":{"displayName":"He Nan Li (Tony)","userId":"01291724548798117557"},"user_tz":420},"id":"IuSCgR729DTb","outputId":"c8e38ff8-aedc-43a5-9f4d-785542cd58d3"},"outputs":[],"source":["# select device\n","if torch.cuda.is_available():\n","    DEVICE = torch.device('cuda')\n","elif torch.backends.mps.is_available():\n","    DEVICE = torch.device('mps')\n","else:\n","    DEVICE = torch.device('cpu')\n","# DEVICE = torch.device('cpu')\n","print(f'{DEVICE=}')\n","\n","# hyper param\n","\n","DATASET = 'CIFAR10'  # CIFAR10 or MNIST\n","print(f'{DATASET=}')\n","if DATASET == 'MNIST':\n","    IMG_SIZE = (32, 32, 1) # original images are padded to make size 32 x 32\n","    DATASET = MNIST\n","elif DATASET == 'CIFAR10':\n","    IMG_SIZE = (32, 32, 3)\n","    DATASET = CIFAR10\n","print(f'{IMG_SIZE=}')\n","\n","BATCH_SIZE = 64\n","N_LAYERS = 8\n","HIDDEN_DIM = 256\n","HIDDEN_DIMS = [HIDDEN_DIM for _ in range(N_LAYERS)]\n","N_TIMESTAMPS = 1000\n","TIMESTAMP_EMBED_DIM = 256\n","# BETA_MIN_MAX = [1e-4, 2e-2]\n","LEARNING_RATE = 5e-5\n","N_EPOCH = 2\n","\n","# prepare data\n","\n","dataset_path = './datasets'\n","transform = transforms.Compose([\n","    transforms.Resize(32), transforms.ToTensor()\n","])  # TODO: try subtract mean & div by std\n","train_dataset = DATASET(dataset_path, transform=transform,\n","                        train=True, download=True)\n","eval_dataset = DATASET(dataset_path, transform=transform,\n","                       train=True, download=True)\n","test_dataset = DATASET(dataset_path, transform=transform,\n","                       train=False, download=True)\n","NUM_DATASET = len(train_dataset)\n","NUM_DATASET = 1000  # use a smaller dataset\n","NUM_TRAIN = int(NUM_DATASET * 0.8)\n","train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE,\n","                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n","# train_sample = list(train_loader)[0][0]\n","\n","# print(train_dataset.data.type())\n","# print(train_dataset.data.float().mean(), train_dataset.data.min(), train_dataset.data.max(), train_dataset.data.shape)\n","# print(train_sample.mean(), train_sample.min(), train_sample.max(), train_sample.shape)\n","\n","eval_loader = DataLoader(dataset=eval_dataset, batch_size=BATCH_SIZE,\n","                         sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, NUM_DATASET)))\n","test_loader = DataLoader(dataset=test_dataset,  batch_size=BATCH_SIZE)\n","\n","N_CLASS = len(train_dataset.classes)\n","print(f'{N_CLASS=} {NUM_TRAIN=}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VhXoUAOHKsJz"},"outputs":[],"source":["IMAGE_CHANNEL = IMG_SIZE[2]\n","eps_model = ddpm.UNet(\n","    image_channels=IMAGE_CHANNEL,\n","    n_channels=64,\n","    ch_mults=[1, 2, 2, 4],\n","    is_attn=[False, False, False, True],\n",").to(DEVICE)\n","\n","diffusion = ddpm.DenoiseDiffusion(\n","    eps_model=eps_model,\n","    n_diffusion_timestep=N_TIMESTAMPS,\n","    device=DEVICE,\n",")\n","\n","optimizer = Adam(eps_model.parameters(), lr=LEARNING_RATE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZJ4H5veqKsJ0","outputId":"c276fa92-5185-4983-d3aa-b8a096bb095e"},"outputs":[],"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(\"Number of model parameters: \", count_parameters(eps_model))\n","print(\"forwarding diffusion sampling...\")\n","eps_model.eval()\n","with torch.no_grad():\n","    for x, class_ in train_loader:\n","        print(x.mean(), x.min(), x.max(), x.shape)\n","        x = x[:8].to(DEVICE)\n","        xt = torch.tensor([]).to(DEVICE)\n","        for t in range(0, N_TIMESTAMPS, 100):\n","            xt = torch.cat((xt, diffusion.q(\n","                x, torch.full((8, ), t).to(DEVICE))))\n","        print(xt.mean(), xt.min(), xt.max(), xt.shape)\n","        utils.draw_sample_image(xt, f\"adding noise\")\n","        break  # only sample 1 batch\n","eps_model.train()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":13787,"status":"error","timestamp":1717182341361,"user":{"displayName":"He Nan Li (Tony)","userId":"01291724548798117557"},"user_tz":420},"id":"fbavZG339DTb","outputId":"70336652-450e-45bc-9b3b-d8705c597b73"},"outputs":[],"source":["print(\"start training\")\n","\n","train_loss_arr = [0]\n","eval_loss_arr = [0]\n","iters = [0]\n","\n","EVAL_EVERY_ITER = 10\n","EVAL_BATCH = 10\n","\n","def evaluate():\n","    with torch.no_grad():\n","        eval_loss = 0.0\n","        train_loss = 0.0\n","        eps_model.eval()\n","        for train_batch_idx, (img_, class_) in enumerate(train_loader):\n","            train_loss += diffusion.loss(img_.to(DEVICE), class_.to(DEVICE)).item()\n","            if train_batch_idx > EVAL_BATCH:\n","                break\n","        for eval_batch_idx, (img_, class_) in enumerate(eval_loader):\n","            eval_loss += diffusion.loss(img_.to(DEVICE), class_.to(DEVICE)).item()\n","            if eval_batch_idx > EVAL_BATCH:\n","                break\n","        eval_loss /= eval_batch_idx\n","        train_loss /= train_batch_idx\n","        eval_loss_arr.append(eval_loss)\n","        train_loss_arr.append(train_loss)\n","        iters.append(iters[-1] + EVAL_EVERY_ITER)\n","        eps_model.train()\n","\n","i = 0\n","for epoch in range(N_EPOCH):\n","    print(f'training {epoch=}')\n","    for batch_idx, (img_, class_) in tqdm(enumerate(train_loader), total=len(train_loader)):\n","        i += 1\n","        if i % EVAL_EVERY_ITER == 0:\n","            evaluate()\n","        \n","        optimizer.zero_grad()\n","        loss = diffusion.loss(img_.to(DEVICE), class_.to(DEVICE))\n","        loss.backward()\n","        optimizer.step()\n","        \n","            \n","plt.figure()\n","plt.plot(iters[1:], train_loss_arr[1:], label = 'train')\n","plt.plot(iters[1:], eval_loss_arr[1:], label = 'validation')\n","plt.xlabel('iters')\n","plt.ylabel('loss')\n","plt.legend()\n","plt.title('training loss vs validation loss')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["eps_model.eval()\n","print('sampling...')\n","class_ = torch.arange(8).to(DEVICE)\n","with torch.no_grad():\n","    x = torch.randn([8, IMAGE_CHANNEL, 32, 32], device=DEVICE)\n","    xt = torch.tensor([]).to(DEVICE)\n","    for t_ in tqdm(range(N_TIMESTAMPS)):\n","        t = N_TIMESTAMPS - t_ - 1\n","        t = x.new_full((1,), t, dtype=torch.long)\n","        x = diffusion.p_sample(x, t, class_)\n","        if t_ % 200 == 0 or t_ == N_TIMESTAMPS - 1:\n","            xt = torch.cat((xt, x))\n","    utils.draw_sample_image(xt, f\"generating\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# FID\n","import torch\n","from torchmetrics.image.fid import FrechetInceptionDistance\n","\n","fid = FrechetInceptionDistance(feature=64, normalize=True)\n","\n","for generated_img in xt:\n","    fid.update(generated_img, real=False)\n","\n","# generated_imgs = v2.ToDtype(torch.uint8, scale=True)(xt) # scaled to [0, 255]\n","\n","for test_img, test_class in enumerate(test_loader):\n","    for i in len(test_img):\n","        print(test_img)\n","        print(test_class)\n","        break\n","    break\n","\n","# \n","# _ = torch.manual_seed(123)\n","# \n","# \n","# generate two slightly overlapping image intensity distributions\n","# imgs_dist1 = torch.randint(0, 200, (100, 3, 299, 299), dtype=torch.uint8)\n","# imgs_dist2 = torch.randint(100, 255, (100, 3, 299, 299), dtype=torch.uint8)\n","# fid.update(imgs_dist1, real=True)\n","# fid.update(imgs_dist2, real=False)\n","# fid.compute()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pJkhiSd49DTc"},"outputs":[],"source":["\n","class SinusoidalPosEmb(nn.Module):\n","    def __init__(self, dim):\n","        super().__init__()\n","        self.dim = dim\n","\n","    def forward(self, x):\n","        device = x.device\n","        half_dim = self.dim // 2\n","        emb = math.log(10000) / (half_dim - 1)\n","        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n","        emb = x[:, None] * emb[None, :]\n","        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n","        return emb\n","\n","\n","class ConvBlock(nn.Conv2d):\n","    \"\"\"\n","        Conv2D Block\n","            Args:\n","                x: (N, C_in, H, W)\n","            Returns:\n","                y: (N, C_out, H, W)\n","    \"\"\"\n","\n","    def __init__(self, in_channels, out_channels, kernel_size, activation_fn=None, drop_rate=0.,\n","                 stride=1, padding='same', dilation=1, groups=1, bias=True, gn=False, gn_groups=8):\n","\n","        if padding == 'same':\n","            padding = kernel_size // 2 * dilation\n","\n","        super(ConvBlock, self).__init__(in_channels, out_channels, kernel_size,\n","                                        stride=stride, padding=padding, dilation=dilation,\n","                                        groups=groups, bias=bias)\n","\n","        self.activation_fn = nn.SiLU() if activation_fn else None\n","        self.group_norm = nn.GroupNorm(gn_groups, out_channels) if gn else None\n","\n","    def forward(self, x, time_embedding=None, residual=False):\n","\n","        if residual:\n","            # in the paper, diffusion timestep embedding was only applied to residual blocks of U-Net\n","            x = x + time_embedding\n","            y = x\n","            x = super(ConvBlock, self).forward(x)\n","            y = y + x\n","        else:\n","            y = super(ConvBlock, self).forward(x)\n","        y = self.group_norm(y) if self.group_norm is not None else y\n","        y = self.activation_fn(y) if self.activation_fn is not None else y\n","\n","        return y\n","\n","\n","class Denoiser(nn.Module):\n","\n","    def __init__(self, image_resolution, hidden_dims=[256, 256], diffusion_time_embedding_dim=256, n_times=1000):\n","        \"\"\"\n","        image_resolution: example array [32, 32, 3]\n","\n","        \"\"\"\n","        super(Denoiser, self).__init__()\n","\n","        _, _, img_C = image_resolution\n","\n","        self.time_embedding = SinusoidalPosEmb(diffusion_time_embedding_dim)\n","\n","        self.in_project = ConvBlock(img_C, hidden_dims[0], kernel_size=7)\n","\n","        self.time_project = nn.Sequential(\n","            ConvBlock(diffusion_time_embedding_dim,\n","                      hidden_dims[0], kernel_size=1, activation_fn=True),\n","            ConvBlock(hidden_dims[0], hidden_dims[0], kernel_size=1))\n","\n","        self.convs = nn.ModuleList([ConvBlock(\n","            in_channels=hidden_dims[0], out_channels=hidden_dims[0], kernel_size=3)])\n","\n","        for idx in range(1, len(hidden_dims)):\n","            self.convs.append(ConvBlock(hidden_dims[idx-1], hidden_dims[idx], kernel_size=3, dilation=3**((idx-1)//2),\n","                                        activation_fn=True, gn=True, gn_groups=8))\n","\n","        self.out_project = ConvBlock(\n","            hidden_dims[-1], out_channels=img_C, kernel_size=3)\n","\n","    def forward(self, perturbed_x, diffusion_t):\n","        y = perturbed_x\n","\n","        diffusion_embedding = self.time_embedding(diffusion_t)\n","        diffusion_embedding = self.time_project(\n","            diffusion_embedding.unsqueeze(-1).unsqueeze(-2))\n","\n","        y = self.in_project(y)\n","\n","        for i in range(len(self.convs)):\n","            y = self.convs[i](y, diffusion_embedding, residual=True)\n","\n","        y = self.out_project(y)\n","\n","        return y\n","\n","\n","model = Denoiser(image_resolution=IMG_SIZE,\n","                 hidden_dims=HIDDEN_DIMS,\n","                 diffusion_time_embedding_dim=TIMESTAMP_EMBED_DIM,\n","                 n_times=N_TIMESTAMPS).to(DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CVd6Xc7m9DTc"},"outputs":[],"source":["class Diffusion(nn.Module):\n","    def __init__(self, model, image_resolution=[32, 32, 3], n_times=1000, beta_minmax=[1e-4, 2e-2], device='cuda'):\n","\n","        super(Diffusion, self).__init__()\n","\n","        self.n_times = n_times\n","        self.img_H, self.img_W, self.img_C = image_resolution\n","\n","        self.model = model\n","\n","        # define linear variance schedule(betas)\n","        beta_1, beta_T = beta_minmax\n","        betas = torch.linspace(start=beta_1, end=beta_T, steps=n_times).to(\n","            device)  # follows DDPM paper\n","        self.sqrt_betas = torch.sqrt(betas)\n","\n","        # define alpha for forward diffusion kernel\n","        self.alphas = 1 - betas\n","        self.sqrt_alphas = torch.sqrt(self.alphas)\n","        alpha_bars = torch.cumprod(self.alphas, dim=0)\n","        self.sqrt_one_minus_alpha_bars = torch.sqrt(1-alpha_bars)\n","        self.sqrt_alpha_bars = torch.sqrt(alpha_bars)\n","\n","        self.device = device\n","\n","    def scale_to_minus_one_to_one(self, x):\n","        # according to the DDPMs paper, normalization seems to be crucial to train reverse process network\n","        return x * 2 - 1\n","\n","    def reverse_scale_to_zero_to_one(self, x):\n","        return (x + 1) * 0.5\n","\n","    def make_noisy(self, x_zeros, t):\n","        # perturb x_0 into x_t (i.e., take x_0 samples into forward diffusion kernels)\n","        epsilon = torch.randn_like(x_zeros).to(self.device)\n","\n","        sqrt_alpha_bar = self.extract(self.sqrt_alpha_bars, t, x_zeros.shape)\n","        sqrt_one_minus_alpha_bar = self.extract(\n","            self.sqrt_one_minus_alpha_bars, t, x_zeros.shape)\n","\n","        # Let's make noisy sample!: i.e., Forward process with fixed variance schedule\n","        #      i.e., sqrt(alpha_bar_t) * x_zero + sqrt(1-alpha_bar_t) * epsilon\n","        noisy_sample = x_zeros * sqrt_alpha_bar + epsilon * sqrt_one_minus_alpha_bar\n","\n","        return noisy_sample.detach(), epsilon\n","\n","    def forward(self, x_zeros):\n","        x_zeros = self.scale_to_minus_one_to_one(x_zeros)\n","\n","        B, _, _, _ = x_zeros.shape\n","\n","        # (1) randomly choose diffusion time-step\n","        t = torch.randint(low=0, high=self.n_times,\n","                          size=(B,)).long().to(self.device)\n","\n","        # (2) forward diffusion process: perturb x_zeros with fixed variance schedule\n","        perturbed_images, epsilon = self.make_noisy(x_zeros, t)\n","\n","        # (3) predict epsilon(noise) given perturbed data at diffusion-timestep t.\n","        pred_epsilon = self.model(perturbed_images, t)\n","\n","        return perturbed_images, epsilon, pred_epsilon\n","\n","    def denoise_at_t(self, x_t, timestep, t):\n","        B, _, _, _ = x_t.shape\n","        if t > 1:\n","            z = torch.randn_like(x_t).to(self.device)\n","        else:\n","            z = torch.zeros_like(x_t).to(self.device)\n","\n","        # at inference, we use predicted noise(epsilon) to restore perturbed data sample.\n","        epsilon_pred = self.model(x_t, timestep)\n","\n","        alpha = self.extract(self.alphas, timestep, x_t.shape)\n","        sqrt_alpha = self.extract(self.sqrt_alphas, timestep, x_t.shape)\n","        sqrt_one_minus_alpha_bar = self.extract(\n","            self.sqrt_one_minus_alpha_bars, timestep, x_t.shape)\n","        sqrt_beta = self.extract(self.sqrt_betas, timestep, x_t.shape)\n","\n","        # denoise at time t, utilizing predicted noise\n","        x_t_minus_1 = 1 / sqrt_alpha * \\\n","            (x_t - (1-alpha)/sqrt_one_minus_alpha_bar*epsilon_pred) + sqrt_beta*z\n","\n","        return x_t_minus_1.clamp(-1., 1)\n","\n","    def sample(self, N):\n","        # start from random noise vector, x_0 (for simplicity, x_T declared as x_t instead of x_T)\n","        x_t = torch.randn((N, self.img_C, self.img_H,\n","                          self.img_W)).to(self.device)\n","\n","        # autoregressively denoise from x_T to x_0\n","        #     i.e., generate image from noise, x_T\n","        for t in range(self.n_times-1, -1, -1):\n","            timestep = torch.tensor([t]).repeat_interleave(\n","                N, dim=0).long().to(self.device)\n","            x_t = self.denoise_at_t(x_t, timestep, t)\n","\n","        # denormalize x_0 into 0 ~ 1 ranged values.\n","        x_0 = self.reverse_scale_to_zero_to_one(x_t)\n","\n","        return x_0\n","\n","\n","diffusion = Diffusion(model, image_resolution=IMG_SIZE, n_times=N_TIMESTAMPS,\n","                      beta_minmax=BETA_MIN_MAX, device=DEVICE).to(DEVICE)\n","\n","optimizer = Adam(diffusion.parameters(), lr=LEARNING_RATE)\n","denoising_loss = nn.MSELoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"roI33T6z9DTd"},"outputs":[],"source":["\n","from torchsummary import summary\n","\n","summary(diffusion, (1, 28, 28))\n","\n","\n","\n","model.eval()\n","for batch_idx, (x, class_) in enumerate(train_loader):\n","    print(x.mean(), x.min(), x.max(), x.shape)\n","    x = x[:8].to(DEVICE)\n","    # perturbed_images, epsilon, pred_epsilon = diffusion(x)\n","    # perturbed_images = diffusion.reverse_scale_to_zero_to_one(perturbed_images)\n","    for t in range(0, N_TIMESTAMPS, 100):\n","        noisy_sample, _ = diffusion.make_noisy(\n","            x, torch.full((8, ), t).to(DEVICE))\n","        # print(f'{len(noisy_sample)}')\n","        # show_image(noisy_sample[0])\n","        draw_sample_image(noisy_sample, f\"adding noise t={t}\")\n","    break\n","\n","# print(f'{len(perturbed_images)=}')\n","# show_image(perturbed_images[0])\n","# show_image(perturbed_images[1])\n","# show_image(perturbed_images[63])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pOjUQ18Z9DTd"},"outputs":[],"source":["model.eval()\n","\n","with torch.no_grad():\n","    generated_images = diffusion.sample(N=8)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pV1Rzzrj9DTe"},"outputs":[],"source":["show_image(generated_images[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dsLxZEHe9DTe"},"outputs":[],"source":["\n","# draw_sample_image(perturbed_images, \"Perturbed Images\")\n","draw_sample_image(generated_images, \"Generated Images\")\n","draw_sample_image(x[:8], \"Ground-truth Images\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DMHSll9M9DTe"},"outputs":[],"source":["from ignite.metrics import InceptionScore\n","from diffusers import DDPMPipeline\n","\n","pipeline = DDPMPipeline.from_pretrained('1aurent/ddpm-mnist').to(DEVICE)\n","print(f'{pipeline.device}')\n","\n","images = torch.tensor([])\n","for i in range(10):\n","    image_tensor = torchvision.transforms.functional.pil_to_tensor(pipeline().images[0])\n","    images = torch.cat((images, image_tensor))\n","    print(images.shape)\n","\n","# metric = InceptionScore()\n","# metric.attach(default_evaluator, \"is\")\n","# y = torch.rand(10, 3, 299, 299)\n","# state = default_evaluator.run([y])\n","# print(state.metrics[\"is\"])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kYz4bR4P9DTe"},"outputs":[],"source":["inception = InceptionScore()\n","# generate some images\n","# imgs = torch.randint(0, 255, (100, 3, 299, 299), dtype=torch.uint8)\n","# print(type(image))\n","for image in images:\n","\n","    image_tensor = torchvision.transforms.functional.pil_to_tensor(image)\n","    print(f'{image_tensor.float().mean()=} {image_tensor.min()=} {image_tensor.max()=} {image_tensor.shape=}')\n","    # print(image_tensor)\n","    # print(image_tensor.expand(1, 3, 28, 28))\n","    expanded_image_tensor = image_tensor.expand(1, 3, 28, 28)\n","    # print(f'{expanded_image_tensor.shape}')\n","    inception.update(expanded_image_tensor)\n","    print(f'{inception.compute()=}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N0oemGQk9DTe"},"outputs":[],"source":["from torchmetrics.image.inception import InceptionScore\n","import torch\n","from torchmetrics.image.fid import FrechetInceptionDistance\n","\n","\n","\n","pure_noise_is = InceptionScore()\n","cifar10_is = InceptionScore()\n","generated_is = InceptionScore()\n","\n","for batch_idx, (x, class_) in tqdm(enumerate(train_loader), total=len(train_loader)):\n","    cifar10_is.update(v2.ToDtype(torch.uint8, scale=True)(x))\n","    generated_is.update(v2.ToDtype(torch.uint8, scale=True)(generated_images))\n","    noise = torch.randint(0, 255, (64, 3, 32, 32), dtype=torch.uint8)\n","    pure_noise_is.update(noise)\n","    print(f'{pure_noise_is.compute()=} {generated_is.compute()=} {cifar10_is.compute()=}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fid = FrechetInceptionDistance(feature=64, normalize=True)\n","\n","\n","\n","\n","# generated_imgs = v2.ToDtype(torch.uint8, scale=True)(xt) # scaled to [0, 255]\n","\n","for test_img, test_class in test_loader:\n","    fid.update(test_img, real=True)\n","    fid.update(generated_images, real=False)\n","    print(fid.compute())\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
